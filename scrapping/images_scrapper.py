import requests
from bs4 import BeautifulSoup
import os
import re
import time

# --- –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø ---

BASE_URL = "https://www.yachtcharterfleet.com"
START_URL = "https://www.yachtcharterfleet.com/charter/superyachts-for-charter"
# –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –¥–ª—è —Å–∫—Ä–∞–ø—ñ–Ω–≥—É –î–õ–Ø –ö–û–ñ–ù–û–ì–û –¢–ò–ü–£ —è—Ö—Ç.
# –ü–æ—Å—Ç–∞–≤—Ç–µ 1 –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è, –ø–æ—Ç—ñ–º –º–æ–∂–µ—Ç–µ –∑–±—ñ–ª—å—à–∏—Ç–∏.
PAGES_TO_SCRAPE_PER_TYPE = 42
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# –ì–æ–ª–æ–≤–Ω–∞ –ø–∞–ø–∫–∞ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤—Å—ñ—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å
IMAGE_BASE_DIR = "yachts"

YACHT_TYPES = {
    1: 'Motor Yachts', 2: 'Sailing Yachts', 3: 'Expedition Yachts',
    4: 'Classic Yachts', 5: 'Open Yachts', 6: 'Catamarans',
    7: 'Sport Fishing', 8: 'Gulet Yachts'
}

# --- –û–°–ù–û–í–ù–Ü –§–£–ù–ö–¶–Ü–á ---

def get_yacht_links(list_url):
    """–ó–±–∏—Ä–∞—î –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –æ–∫—Ä–µ–º–∏—Ö —è—Ö—Ç –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Å–ø–∏—Å–∫—É."""
    links = []
    try:
        response = requests.get(list_url, headers=HEADERS)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        results_container = soup.find('div', id='yacht-results-listing')
        if results_container:
            yacht_cards = results_container.find_all('div', class_='jsYachtSearchResult')
            for card in yacht_cards:
                link_tag = card.find('a', class_='searchImageLink', href=True)
                if link_tag:
                    links.append(BASE_URL + link_tag['href'])
    except requests.exceptions.RequestException as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Ç—ñ –¥–æ {list_url}: {e}")
    return links

def download_image(url, folder_path, filename):
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –æ–¥–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–∞ URL —ñ –∑–±–µ—Ä—ñ–≥–∞—î –π–æ–≥–æ.
    (–û–ù–û–í–õ–ï–ù–û: –ü—Ä–æ–ø—É—Å–∫–∞—î, —è–∫—â–æ —Ñ–∞–π–ª –≤–∂–µ —ñ—Å–Ω—É—î)
    """
    full_path = os.path.join(folder_path, filename)
    
    # --- –ì–û–õ–û–í–ù–ê –ó–ú–Ü–ù–ê ---
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ñ–∞–π–ª –≤–∂–µ —ñ—Å–Ω—É—î –∑–∞ —Ü–∏–º —à–ª—è—Ö–æ–º
    if os.path.exists(full_path):
        print(f"      ‚úÖ –í–∂–µ —ñ—Å–Ω—É—î: {filename}")
        return # –ù–µ–≥–∞–π–Ω–æ –≤–∏—Ö–æ–¥–∏–º–æ –∑ —Ñ—É–Ω–∫—Ü—ñ—ó
    # ---------------------

    try:
        img_response = requests.get(url, stream=True, headers=HEADERS, timeout=15)
        img_response.raise_for_status()
        
        # –ó–∞–ø–æ–±—ñ–≥–∞—î–º–æ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—é HTML-—Å—Ç–æ—Ä—ñ–Ω–æ–∫ –ø–æ–º–∏–ª–æ–∫ —è–∫ –∑–æ–±—Ä–∞–∂–µ–Ω—å
        content_type = img_response.headers.get('Content-Type')
        if 'image' not in content_type:
            print(f"      ‚ùå –ü–æ–º–∏–ª–∫–∞: URL –Ω–µ —î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è–º ({content_type}): {url}")
            return

        with open(full_path, 'wb') as f:
            for chunk in img_response.iter_content(8192):
                f.write(chunk)
        print(f"      ‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –Ω–æ–≤–µ: {filename}")
        
    except requests.exceptions.RequestException as e:
        print(f"      ‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {url}: {e}")

def download_images_for_yacht(yacht_url):
    """
    –ó–Ω–∞—Ö–æ–¥–∏—Ç—å —ñ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î –≤—Å—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –¥–ª—è –æ–¥–Ω—ñ—î—ó —è—Ö—Ç–∏.
    (–û–ù–û–í–õ–ï–ù–û: –®—É–∫–∞—î –≤ –î–í–û–• –º–æ–∂–ª–∏–≤–∏—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞—Ö –¥–ª—è –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö —Ñ–æ—Ç–æ)
    """
    try:
        response = requests.get(yacht_url, headers=HEADERS)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # 1. –û—Ç—Ä–∏–º—É—î–º–æ –Ω–∞–∑–≤—É —è—Ö—Ç–∏ –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–∞–ø–∫–∏
        h1_tag = soup.find('h1')
        if h1_tag:
            full_name = h1_tag.text.strip()
            yacht_name = re.sub(r'\s+YACHT.*', '', full_name, flags=re.IGNORECASE).strip()
        else:
            yacht_name = f"unknown_yacht_{int(time.time())}"
        
        sanitized_name = re.sub(r'[^\w\s-]', '', yacht_name).strip()
        sanitized_name_upper = sanitized_name.upper()
        
        yacht_folder_path = os.path.join(IMAGE_BASE_DIR, sanitized_name_upper)
        os.makedirs(yacht_folder_path, exist_ok=True)
        # print(f"   –û–±—Ä–æ–±–∫–∞ –ø–∞–ø–∫–∏: {yacht_folder_path}")

        # 2. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –≥–æ–ª–æ–≤–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (–∑ –ø–æ–¥–≤—ñ–π–Ω–æ—é –ª–æ–≥—ñ–∫–æ—é)
        main_image_tag = soup.find('img', id='overview_image')
        if not main_image_tag:
            main_image_tag = soup.find('img', attrs={'data-image-name': re.compile(yacht_name, re.IGNORECASE)})

        if main_image_tag and main_image_tag.has_attr('src'):
            main_image_url = main_image_tag['src']
            if not main_image_url.startswith('http'):
                main_image_url = BASE_URL + main_image_url
            print(f"    -> –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≥–æ–ª–æ–≤–Ω–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (00_main.jpg)...")
            download_image(main_image_url, yacht_folder_path, "00_main.jpg")
        else:
            print(f"    ‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≥–æ–ª–æ–≤–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –¥–ª—è {yacht_name}")

        
        # 3. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è (‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–ò–ô –°–ï–õ–ï–ö–¢–û–†)
        
        # --- –ì–û–õ–û–í–ù–ê –ó–ú–Ü–ù–ê ---
        # –°–ø–æ—á–∞—Ç–∫—É —à—É–∫–∞—î–º–æ –æ–¥–∏–Ω —Ç–∏–ø –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        additional_images_container = soup.find('div', class_='jsReplaceSlidesHereForMobile')
        
        # –Ø–∫—â–æ –π–æ–≥–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, —à—É–∫–∞—î–º–æ –¥—Ä—É–≥–∏–π —Ç–∏–ø (—è–∫–∏–π –≤–∏ –∑–Ω–∞–π—à–ª–∏)
        if not additional_images_container:
            additional_images_container = soup.find('div', class_='jsTakeImagesFromHere')
        # ---------------------

        if additional_images_container:
            # –¢–µ–ø–µ—Ä —à—É–∫–∞—î–º–æ 'lightbox' –¢–Ü–õ–¨–ö–ò –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –∑–Ω–∞–π–¥–µ–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
            image_links = additional_images_container.find_all('a', class_='lightbox')
            
            if image_links:
                print(f"    -> –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ {len(image_links)} –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å...")
                for i, link_tag in enumerate(image_links):
                    if link_tag.has_attr('href'):
                        image_url = link_tag['href']
                        
                        if not image_url.startswith('http'):
                            image_url = BASE_URL + image_url
                            
                        file_extension = os.path.splitext(image_url.split('?')[0])[1]
                        if not file_extension or len(file_extension) > 5:
                            file_extension = ".jpg" 
                        
                        filename = f"{i+1:02d}{file_extension}"
                        download_image(image_url, yacht_folder_path, filename)
            else:
                 print("    -> –ó–Ω–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä, –∞–ª–µ –≤ –Ω—å–æ–º—É 0 –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å.")
        else:
            print("    -> –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –¥–æ–¥. –∑–æ–±—Ä–∞–∂–µ–Ω—å (–Ω—ñ 'jsReplaceSlidesHereForMobile', –Ω—ñ 'jsTakeImagesFromHere').")
            
    except requests.exceptions.RequestException as e:
        print(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É {yacht_url}: {e}")
    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {yacht_url}: {e}")

# --- –ì–û–õ–û–í–ù–ò–ô –°–ö–†–ò–ü–¢ ---
if __name__ == "__main__":
    # –°—Ç–≤–æ—Ä—é—î–º–æ –≥–æ–ª–æ–≤–Ω—É –ø–∞–ø–∫—É, —è–∫—â–æ —ó—ó –Ω–µ–º–∞—î
    os.makedirs(IMAGE_BASE_DIR, exist_ok=True)
    print(f"–ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –±—É–¥—É—Ç—å –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤ –ø–∞–ø–∫—É: '{IMAGE_BASE_DIR}'")

    # –ì–æ–ª–æ–≤–Ω–∏–π —Ü–∏–∫–ª –ø–æ —Ç–∏–ø–∞—Ö —è—Ö—Ç
    for type_id, type_name in YACHT_TYPES.items():
        print(f"\n{'='*40}\n scraping Category: {type_name}\n{'='*40}")
        
        # –¶–∏–∫–ª –ø–æ —Å—Ç–æ—Ä—ñ–Ω–∫–∞—Ö –¥–ª—è –¥–∞–Ω–æ–≥–æ —Ç–∏–ø—É
        for page_num in range(1, PAGES_TO_SCRAPE_PER_TYPE + 1):
            url = f"{START_URL}?page={page_num}&yacht_type_id_list={type_id}&sort_by=relevance"
            print(f"–û–±—Ä–æ–±–∫–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {url}")
            
            links_on_page = get_yacht_links(url)
            
            if not links_on_page:
                print(f"–ù–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {page_num} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —è—Ö—Ç. –ü–µ—Ä–µ—Ö–æ–¥–∏–º–æ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó.")
                break 
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –¥–ª—è –∫–æ–∂–Ω–æ—ó –∑–Ω–∞–π–¥–µ–Ω–æ—ó —è—Ö—Ç–∏
            for link in links_on_page:
                print(f"  –û–±—Ä–æ–±–∫–∞ —è—Ö—Ç–∏: {link}")
                download_images_for_yacht(link)
                time.sleep(1) # –ù–µ–≤–µ–ª–∏–∫–∞ –∑–∞—Ç—Ä–∏–º–∫–∞
        
        print(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ —Ä–æ–±–æ—Ç—É –∑ –∫–∞—Ç–µ–≥–æ—Ä—ñ—î—é '{type_name}'.")
        time.sleep(2)
        
    print("\n\nüéâ –í—Å—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
